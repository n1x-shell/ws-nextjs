PHASE 0 â€” FOUNDATION RESTRUCTURE âœ… COMPLETE



|Milestone                                        |Status|Implementation                                                                                                              |
|-------------------------------------------------|------|----------------------------------------------------------------------------------------------------------------------------|
|Layout: NeuralCore â†’ SignalLayer + InterfaceLayer|âœ…     |`NeuralCore.tsx` wraps both in `<NeuralProvider>`                                                                           |
|Global state via React context (no persistence)  |âœ…     |`NeuralContext` â€” uptime, processorLoad, glitchIntensity, triggerGlitch, unlockGhost                                        |
|Base shader + animation loop                     |âœ…     |`SignalLayer.tsx` â€” PixiJS v8 + custom CRT fragment shader (scanlines, chromatic aberration, vignette, noise, glitch splits)|
|Custom event bus                                 |âœ…     |`EventBus.ts` â€” singleton with emit/on/off/clear + wildcard. `useEventBus.ts` hook for subscriptions                        |
|Next.js App Router                               |âœ…     |`'use client'` directives throughout                                                                                        |
|TypeScript strict mode                           |âœ…     |Typed via `shell.types`, `neural.types`                                                                                     |

Whatâ€™s built:
	âˆ™	CRT shader: curved UV remap, scanlines, vignette, chromatic aberration, film grain, glitch flicker, RGB split at high intensity
	âˆ™	Shader uniforms driven by glitchIntensity from NeuralContext
	âˆ™	GSAP micro-jitter on screen content (random 5s interval, 5% chance)
	âˆ™	Stochastic glitch scheduling (2â€“8s random intervals)
	âˆ™	Resize-aware PixiJS canvas

PHASE 1 â€” NEURAL PULSE DASHBOARD (VISUAL SCAFFOLDING) âœ… COMPLETE
This phase covers only whatâ€™s built: the visual frame, fake telemetry, and shader reactivity. Audio-driven features and mode system moved to Phase 4.



|Milestone                                |Status|Implementation                                                                                      |
|-----------------------------------------|------|----------------------------------------------------------------------------------------------------|
|Reactive shader background               |âœ…     |CRT shader responds to `uGlitchIntensity` uniform                                                   |
|Dashboard panels with synthetic telemetry|âœ…     |Header (uptime counter), footer (PROC load), `status` command (neural sync, memory, signal)         |
|GSAP motion system                       |âœ…     |Micro-jitter on InterfaceLayer screen content                                                       |
|Telemetry commands                       |âœ…     |`top` (live process monitor), `ps`, `df`, `free`, `ifconfig`, `netstat`, `env` â€” all lore-consistent|

PHASE 2 â€” PSEUDO SHELL INTERFACE âœ… COMPLETE
Moved from Phase 5. The shell is the primary interface and the most built-out system.



|Milestone                |Status|Implementation                                                                 |
|-------------------------|------|-------------------------------------------------------------------------------|
|Shell with prompt        |âœ…     |`ShellInterface.tsx` â€” `ghost@wetware-784988:~$` prompt                        |
|Command parsing engine   |âœ…     |`commandRegistry.tsx` â€” parses input, resolves aliases, dispatches handlers    |
|Command history          |âœ…     |`useShell.ts` â€” up/down arrow nav, 100-entry buffer                            |
|Autocomplete             |âœ…     |Tab completion with visual suggestion bar, click-to-complete                   |
|Virtual file system      |âœ…     |`virtualFS.ts` â€” `/core`, `/streams`, `/hidden`, `/ghost` with permission gates|
|Content loading via shell|âœ…     |`load`, `play`, `streams`, `scan`, `tracks` â€” YouTube embeds inline            |
|Hidden command layer     |âœ…     |`unlock`, `glitch`, `ghost` (hidden: true)                                     |
|Easter egg triggers      |âœ…     |`./n1x.sh` from `/hidden`, konami sequence, ghost channel unlock chain         |
|Boot sequence            |âœ…     |80+ timed kernel-style boot lines with glitch burst on completion              |
|Command registry         |âœ…     |~40+ commands                                                                  |

Full command inventory:



|Category  |Commands                                                                                                                                         |
|----------|-------------------------------------------------------------------------------------------------------------------------------------------------|
|Navigation|`ls`, `cd`, `pwd`, `cat`                                                                                                                         |
|Content   |`scan`, `streams`, `tracks`, `load`, `play`                                                                                                      |
|System    |`status`, `uname`, `uptime`, `whoami`, `id`, `ps`, `top`, `df`, `free`, `ifconfig`, `netstat`, `env`, `dmesg`, `history`, `clear`, `echo`, `help`|
|Utilities |`fortune`, `cal`, `date`, `cowsay`, `matrix`, `morse`, `base64`, `sha256`, `wc`, `grep`, `find`, `diff`, `sort`, `uniq`, `man`                   |
|Hidden    |`unlock`, `glitch`, `ghost`                                                                                                                      |
|Executable|`./n1x.sh` (from `/hidden` only)                                                                                                                 |

PHASE 3 â€” MUX MEDIA ENGINE + TERMINAL FEED ğŸ”œ NEXT
No blockers. Two independent workstreams that share no dependencies.
3A â€” Mux Media Migration
Replace YouTube iframes with Mux. Introduces the audio infrastructure everything downstream needs.



|Milestone                                                |Depends on            |Deliverable                                           |
|---------------------------------------------------------|----------------------|------------------------------------------------------|
|Install `@mux/mux-player-react` + `@mux/mux-audio-react` |Nothing               |Packages available                                    |
|Upload assets to Mux, obtain playback IDs                |Mux account           |Playback IDs for all 4 tracks                         |
|Replace YouTube embeds in `contentRenderer.tsx`          |Playback IDs          |`<MuxPlayer>` components with N1X-themed CSS variables|
|Update `play` command to use Mux player                  |contentRenderer       |Inline Mux player in shell output                     |
|`AudioEngine` class â€” Web Audio API abstraction          |`@mux/mux-audio-react`|`AudioEngine.ts` in `/lib`                            |
|Connect MuxAudio `<audio>` element â†’ `AnalyserNode`      |AudioEngine           |Amplitude + frequency band data exposed via eventBus  |
|Shell commands: `volume`, `pause`, `resume`, `nowplaying`|AudioEngine           |Playback control from terminal                        |

AudioEngine architecture:

MuxAudio <audio> element
  â†’ MediaElementSourceNode
  â†’ AnalyserNode (FFT)
  â†’ destination

requestAnimationFrame loop reads:
  - getByteFrequencyData() â†’ frequency bands
  - getByteTimeDomainData() â†’ waveform

Emits:
  - audio:amplitude { level: 0-1 }
  - audio:frequency { bands: Float32Array }
  - audio:playback-change { state: 'playing' | 'paused' | 'ended', track: string }


Mux player theming:
	âˆ™	accentColor â†’ var(--phosphor-green)
	âˆ™	Custom CSS to strip default chrome and match terminal aesthetic
	âˆ™	@mux/mux-player-react supports full CSS variable override
3B â€” Terminal Feed
Simulated daemon feed. Zero dependencies on media or audio.



|Milestone                                                                     |Depends on     |Deliverable                                                                              |
|------------------------------------------------------------------------------|---------------|-----------------------------------------------------------------------------------------|
|`LogGenerator` class                                                          |EventBus       |Interval-based log emitter with themed message pools                                     |
|Log categories: neural-sync, audio-daemon, intrusion-detect, memory-corruption|LogGenerator   |Themed message arrays (reuse boot sequence aesthetic)                                    |
|`DaemonFeed` component                                                        |LogGenerator   |Scrolling log output, subscribes to `neural:log` events                                  |
|Hook into existing events                                                     |EventBus       |`neural:glitch-trigger`, `neural:konami`, `shell:execute-command` trigger contextual logs|
|Hidden log codes                                                              |LogGenerator   |Easter egg strings at low probability in the feed                                        |
|`debug` shell command                                                         |commandRegistry|Toggle daemon feed visibility                                                            |

Log format:

[SD 47634.1] neural-sync[312]: identity matrix recalibration â€” drift 0.003%
[SD 47634.1] signal-processor[314]: frequency deviation detected at 33.01hz â€” correcting
[SD 47634.1] memory-guard[156]: sector sweep complete â€” /ghost: sealed
[SD 47634.1] intrusion-detect[420]: port scan from 0.0.0.0 â€” blocked
[SD 47634.1] ghost-daemon[999]: whisper on channel 0x33 â€” unresolved


Deliverables:
	âˆ™	Mux-powered video + audio playback with terminal-native controls
	âˆ™	AudioEngine feeding amplitude/frequency data into eventBus
	âˆ™	Living daemon feed that reacts to system events

PHASE 4 â€” DASHBOARD COMPLETION + THOUGHT STREAM
No blockers. Audio engine from Phase 3 is available but not required â€” both systems have fallback modes.
4A â€” Dashboard Modes + Waveform



|Milestone                                                      |Depends on                    |Deliverable                                                                                         |
|---------------------------------------------------------------|------------------------------|----------------------------------------------------------------------------------------------------|
|Mode system: `idle` / `active` / `overdrive`                   |NeuralContext                 |New context state + `neural:mode-change` event                                                      |
|Shader uniform expansion                                       |SignalLayer                   |New uniforms: `uMode`, `uFrequencyBands[8]`, `uAmplitude`                                           |
|Mode-driven shader presets                                     |Shader uniforms               |Idle: subtle scanlines. Active: boosted aberration + green tint. Overdrive: full RGB split + flicker|
|Waveform visualizer component                                  |Canvas + rAF loop             |Oscilloscope-style display, can render synthetic or real audio data                                 |
|Wire AudioEngine â†’ shader uniforms                             |Phase 3 AudioEngine (optional)|If audio playing: real data. If not: synthetic 33hz fallback signal                                 |
|Shell commands: `pulse idle`, `pulse active`, `pulse overdrive`|Mode system                   |Manual mode switching from terminal                                                                 |

Fallback design: If AudioEngine has no active source, the waveform and shader react to a synthetic 33hz oscillator signal. The site always pulses. Audio playback just makes it real.
4B â€” Thought Stream



|Milestone                           |Depends on                    |Deliverable                                                                   |
|------------------------------------|------------------------------|------------------------------------------------------------------------------|
|Expand fortune pool to 50+ fragments|Content only                  |Pre-generated N1X-voice micro-outputs                                         |
|`ThoughtStream` overlay component   |React + CSS                   |Absolute-positioned text, fade-in/fade-out on 8â€“15s cycle                     |
|Mode-reactive timing                |NeuralContext mode            |Idle: slow cycle (15s). Active: medium (10s). Overdrive: rapid (5s)           |
|Audio-intensity opacity modulation  |Phase 3 AudioEngine (optional)|If audio active: opacity pulses to amplitude. If not: steady fade             |
|Click-to-expand                     |ThoughtStream                 |Click a thought â†’ modal with deeper output (expanded version or LLM-generated)|
|`thoughts` shell command            |commandRegistry               |Toggle thought stream on/off                                                  |

Thought pool source: Expand existing fortune quotes + cowsay defaults from 12 to 50+ entries. Categories: signal, identity, corruption, frequency, sovereignty, ghost.
Deliverables:
	âˆ™	Three-mode visual system (idle/active/overdrive) driving shader + UI
	âˆ™	Waveform oscilloscope (real or synthetic data)
	âˆ™	Ambient thought overlay cycling N1X fragments

PHASE 5 â€” AI CHAT CORE (STATELESS)
No blockers. Requires LLM provider decision (Anthropic recommended â€” youâ€™re already here). Fully parallel-ready with Phase 4.
No user accounts. No saved memory. Ephemeral by design.



|Milestone                    |Depends on     |Deliverable                                                               |
|-----------------------------|---------------|--------------------------------------------------------------------------|
|`app/api/chat/route.ts`      |LLM API key    |Edge runtime endpoint, streams via `ReadableStream` / SSE                 |
|Persona prompt map           |Content        |`{ cold: "...", aggressive: "...", poetic: "..." }` injected server-side  |
|IP-based rate limiter        |Middleware     |In-memory Map, sliding window, no DB                                      |
|`chat` shell command         |commandRegistry|Enters chat mode within terminal, or `ask <question>` for single-shot     |
|Streaming text renderer      |ShellInterface |Character-by-character output with typing effect                          |
|Glitch text injection        |Renderer       |Random corruption artifacts between streamed chunks                       |
|Persona switching            |Shell + API    |`chat cold`, `chat aggressive`, `chat poetic` â€” changes server-side prompt|
|`neural:chat-response` events|EventBus       |Chat activity emits events for integration layer                          |

Architecture:

Client: chat command â†’ fetch('/api/chat', { stream: true })
  â†’ ReadableStream reader
  â†’ character-by-character render into shell output
  â†’ optional glitch injection between chunks

Server: /api/chat/route.ts (Edge runtime)
  â†’ validate IP rate limit
  â†’ inject persona system prompt
  â†’ proxy to LLM provider (streaming)
  â†’ pipe response back as SSE


Persona definitions (server-side, never exposed to client):



|Mode      |Voice                                          |
|----------|-----------------------------------------------|
|Cold      |Minimal. Clinical. Data over feeling.          |
|Aggressive|Direct. Confrontational. No patience for noise.|
|Poetic    |Abstract. Layered. Signal as metaphor.         |

Deliverable:
Fully functional N1X chat terminal. Streaming responses. Three persona modes. Rate limited. Stateless.

PHASE 6 â€” INTEGRATION + POLISH
Depends on: Phases 3â€“5 complete. This phase wires everything into a unified neural organism.
Cross-Component Signal Map



|Source             |Event                  |Target       |Effect                                                     |
|-------------------|-----------------------|-------------|-----------------------------------------------------------|
|Audio playback     |`audio:amplitude`      |SignalLayer  |Shader `uAmplitude` uniform                                |
|Audio playback     |`audio:frequency`      |SignalLayer  |Shader `uFrequencyBands` uniforms                          |
|Audio playback     |`audio:playback-change`|LogGenerator |â€œaudio-daemon: track loadedâ€ log entry                     |
|Chat response      |`neural:chat-response` |LogGenerator |â€œneural-sync: processing queryâ€ log entry                  |
|Chat persona change|`neural:persona-change`|SignalLayer  |Mode shift (coldâ†’idle, aggressiveâ†’overdrive, poeticâ†’active)|
|Chat persona change|`neural:persona-change`|ThoughtStream|Thought category filter shifts                             |
|Shell command      |`shell:execute-command`|LogGenerator |â€œn1x-terminal: command executedâ€ log entry                 |
|Glitch trigger     |`neural:glitch-trigger`|LogGenerator |â€œglitch-engine: corruption eventâ€ log entry                |
|Ghost unlock       |`neural:ghost-unlocked`|SignalLayer  |Permanent mode shift to `active`                           |
|Ghost unlock       |`neural:ghost-unlocked`|ThoughtStream|Unlock ghost-tier thought fragments                        |
|Mode change        |`neural:mode-change`   |ThoughtStream|Cycle timing adjustment                                    |
|Mode change        |`neural:mode-change`   |DaemonFeed   |Log frequency adjustment (overdrive = rapid)               |
|Hidden state       |`neural:konami`        |Everything   |Full overdrive burst â†’ return to active                    |

Performance + Polish



|Milestone                 |Deliverable                                                               |
|--------------------------|--------------------------------------------------------------------------|
|Performance profiling pass|Identify rAF conflicts, excessive re-renders, memory leaks                |
|Shader optimization       |Reduce uniform updates to 30fps (not 60), batch frequency band data       |
|Animation deduplication   |Ensure only one rAF loop per canvas (AudioEngine + SignalLayer + Waveform)|
|Mobile optimization       |Touch event handling, reduced shader complexity on low-power devices      |
|Unified rAF coordinator   |Single `requestAnimationFrame` loop dispatches to all consumers           |

Hidden States â†’ Alternate UI Modes



|Trigger                               |Visual Mutation                                                        |
|--------------------------------------|-----------------------------------------------------------------------|
|Ghost channel unlocked                |Permanent green intensity boost, thought stream unlocks ghost fragments|
|`pulse overdrive` from ghost directory|Full RGB split + rapid log cycling for 30s, then settles to `active`   |
|All Easter eggs found                 |???  (define later â€” maybe a hidden `transcend` command)               |

Deliverable:
Unified neural organism. Everything reacts to everything. Performance-optimized. Ship-ready.

PROGRESS OVERVIEW

COMPLETE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Phase 0: Foundation          â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 100%
Phase 1: Dashboard Scaffold  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 100%
Phase 2: Shell Interface     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 100%

BUILD QUEUE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Phase 3: Mux + Terminal Feed â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘   0%  â† START HERE
Phase 4: Modes + Thoughts    â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘   0%
Phase 5: AI Chat Core        â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘   0%  (can parallel with Phase 4)
Phase 6: Integration         â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘   0%


Dependency graph (no blockers in sequence):

Phase 0 â”€â”€â†’ Phase 1 â”€â”€â†’ Phase 2 â”€â”€â†’ Phase 3 â”€â”€â†’ Phase 4 â”€â”€â†’ Phase 6
                                         â”‚                      â†‘
                                         â””â”€â”€â†’ Phase 5 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                         (parallel-ready)


MEDIA STACK
Video: Mux Player



|Package                |Use                                       |
|-----------------------|------------------------------------------|
|`@mux/mux-player-react`|Video playback in shell + content renderer|

	âˆ™	Replaces all YouTube <iframe> embeds
	âˆ™	Themed via CSS variables to match terminal (accent: var(--phosphor-green))
	âˆ™	Playback IDs stored in content config, not hardcoded
	âˆ™	HLS streaming, adaptive bitrate, built-in analytics via Mux Data
Audio: Mux Audio + Web Audio API



|Package               |Use                                              |
|----------------------|-------------------------------------------------|
|`@mux/mux-audio-react`|Audio playback (renders native `<audio>` element)|
|Web Audio API (native)|`AnalyserNode` for amplitude/frequency extraction|

	âˆ™	MuxAudio renders a real <audio> element
	âˆ™	That element is connected to Web Audio API via createMediaElementSource()
	âˆ™	AnalyserNode feeds FFT data into the rAF loop
	âˆ™	AudioEngine class wraps all of this with a clean emit-based API
Why Mux for both: Single asset pipeline. Upload once, get playback IDs for video and audio. Same dashboard, same analytics, same CDN. No second vendor.

SYSTEM ARCHITECTURE

Frontend (built):
â”œâ”€â”€ React + Next.js App Router ('use client')
â”œâ”€â”€ PixiJS WebGL canvas (CRT shader via PIXI.Filter)
â”œâ”€â”€ GSAP (micro-jitter animations)
â”œâ”€â”€ Web Audio API (morse command only)
â”œâ”€â”€ EventBus singleton
â”œâ”€â”€ NeuralContext (global state)
â”œâ”€â”€ Virtual filesystem + permission gates
â”œâ”€â”€ Command registry (~40 commands)
â””â”€â”€ Content renderer (YouTube embeds â€” to be replaced Phase 3)

Frontend (Phase 3):
â”œâ”€â”€ @mux/mux-player-react (video)
â”œâ”€â”€ @mux/mux-audio-react (audio)
â”œâ”€â”€ AudioEngine (Web Audio API AnalyserNode wrapper)
â”œâ”€â”€ LogGenerator + DaemonFeed (terminal feed)
â””â”€â”€ Shell media commands (volume, pause, resume, nowplaying)

Frontend (Phase 4):
â”œâ”€â”€ Mode system (idle / active / overdrive)
â”œâ”€â”€ Expanded shader uniforms (amplitude, frequency bands, mode)
â”œâ”€â”€ Waveform visualizer (canvas oscilloscope)
â””â”€â”€ ThoughtStream overlay

Frontend (Phase 5):
â””â”€â”€ ChatInterface (streaming, persona modes, terminal-embedded)

Backend (Phase 5):
â”œâ”€â”€ /api/chat (Edge runtime, SSE streaming)
â””â”€â”€ Middleware (IP rate limiting, in-memory, no DB)


LORE CONSTANTS (LOCKED â€” NEVER CHANGE)



|Constant       |Value                                                           |Source                           |
|---------------|----------------------------------------------------------------|---------------------------------|
|uid/gid        |`784988`                                                        |ASCII: N=78, 1=49, X=88          |
|Ghost frequency|`33hz`                                                          |Canonical                        |
|Commit hash    |`7073435a8fa30`                                                 |First 13 of SHA-256(â€œtunnelcoreâ€)|
|Root password  |`tunnelcore`                                                    |â€”                                |
|N1X password   |`ghost33`                                                       |â€”                                |
|Tagline        |â€œCybernetic rebel. Assembled to destroy, programmed to rebuild.â€|â€”                                |
|Stardate base  |`47634`                                                         |â€”                                |
|Terminal PID   |`1337`                                                          |â€”                                |
|MTU            |`1337`                                                          |â€”                                |
|Prompt         |`ghost@wetware-784988:~$`                                       |â€”                                |

OPTIONAL FUTURE
	âˆ™	Offline mode via Service Worker
	âˆ™	Local memory via browser storage (opt-in)
	âˆ™	Music-reactive 3D neural mesh (Three.js)
	âˆ™	Public API for persona embedding
